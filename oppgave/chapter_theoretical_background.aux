\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Moon2002}
\citation{Worboys2003}
\citation{Achanta2012}
\citation{Achanta2012}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theoretical background}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Shape detection}{7}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Shape detection using superpixels}{7}{section.2.2}}
\citation{Achanta2012}
\citation{Achanta2012}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualization of image segmentation using SLIC \citep  {Achanta2012}}}{8}{figure.2.1}}
\newlabel{fig:superpixels}{{2.1}{8}{Visualization of image segmentation using SLIC \citep {Achanta2012}}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Simple Linear Iterative Clustering (SLIC)}{8}{subsection.2.2.1}}
\citation{Lary2016}
\citation{Minsky1969}
\newlabel{eq:distanceSuperpixel}{{2.1}{9}{Simple Linear Iterative Clustering (SLIC)}{equation.2.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{9}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Feed-forward neural networks}{9}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {paragraph}{Input Layer}{10}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Hidden layer}{10}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{Output layer}{10}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Training a neural network}{10}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation learning}{10}{section*.12}}
\citation{Patterson2017}
\citation{Patterson2017}
\newlabel{eq:backupdate}{{2.2}{11}{Backpropagation learning}{equation.2.3.2}{}}
\newlabel{eq:activation}{{2.3}{11}{Backpropagation learning}{equation.2.3.3}{}}
\newlabel{eq:inputsum}{{2.4}{11}{Backpropagation learning}{equation.2.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The two last layers of an multilayer feed-forward neural network \cite  {Patterson2017}}}{11}{figure.2.2}}
\newlabel{fig:backpropagationtwolayers}{{2.2}{11}{The two last layers of an multilayer feed-forward neural network \cite {Patterson2017}}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Activation functions}{12}{subsection.2.3.3}}
\@writefile{toc}{\contentsline {paragraph}{Linear}{12}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Sigmoid}{12}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Tanh (and Hard Tanh)}{12}{section*.15}}
\@writefile{toc}{\contentsline {paragraph}{Softmax}{12}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{ReLU}{12}{section*.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Loss functions}{13}{subsection.2.3.4}}
\@writefile{toc}{\contentsline {paragraph}{Hinge loss}{13}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{Logistic loss}{13}{section*.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Hyperparamenters}{13}{subsection.2.3.5}}
\newlabel{section:hyperparameters}{{2.3.5}{13}{Hyperparamenters}{subsection.2.3.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Learning rate}{13}{section*.20}}
\citation{Patterson2017}
\@writefile{toc}{\contentsline {paragraph}{Regularization}{14}{section*.21}}
\@writefile{toc}{\contentsline {paragraph}{Momentum}{14}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{Sparsity}{14}{section*.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Convolutional neural networks}{14}{subsection.2.3.6}}
\@writefile{toc}{\contentsline {subsubsection}{Biological Inspiration}{14}{section*.24}}
\@writefile{toc}{\contentsline {subsubsection}{Difference from regular feed-forward multilayer neural networks}{14}{section*.25}}
\citation{Patterson2017}
\@writefile{toc}{\contentsline {subsubsection}{Architecture}{15}{section*.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A general presentation of the architecture of CNNs \cite  {Patterson2017}}}{15}{figure.2.3}}
\newlabel{fig:cnnarchitecture}{{2.3}{15}{A general presentation of the architecture of CNNs \cite {Patterson2017}}{figure.2.3}{}}
\citation{Stanford2017}
\citation{Cambridge2017}
\citation{Karpathy2017}
\@writefile{toc}{\contentsline {subsubsection}{Convolutional layers}{16}{section*.27}}
\newlabel{eq:convop}{{2.5}{16}{Convolutional layers}{equation.2.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The convolution operation (applying a kernel filter) \cite  {Cambridge2017}}}{16}{figure.2.4}}
\newlabel{fig:cnnarchitecture}{{2.4}{16}{The convolution operation (applying a kernel filter) \cite {Cambridge2017}}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{ReLU layers}{16}{section*.28}}
\@writefile{toc}{\contentsline {subsubsection}{Pooling layers}{17}{section*.29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Example of max pooling operation \cite  {Karpathy2017}}}{17}{figure.2.5}}
\newlabel{fig:maxpooling}{{2.5}{17}{Example of max pooling operation \cite {Karpathy2017}}{figure.2.5}{}}
\@setckpt{chapter_theoretical_background}{
\setcounter{page}{18}
\setcounter{equation}{5}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{0}
\setcounter{r@tfl@t}{0}
\setcounter{parentequation}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{2}
\setcounter{bookmark@seq@number}{21}
\setcounter{section@level}{3}
}
