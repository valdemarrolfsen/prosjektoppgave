Automatically generated by Mendeley Desktop 1.17.12
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{ESA,
author = {ESA},
title = {{Overview - Copernicus}},
url = {http://www.esa.int/Our{\_}Activities/Observing{\_}the{\_}Earth/Copernicus/Overview3},
urldate = {2017-10-30}
}
@misc{Stanford2017,
author = {Stanford},
title = {{Sliding Windows - Stanford University}},
url = {https://www.coursera.org/learn/machine-learning/lecture/bQhq3/sliding-windows},
urldate = {2017-11-14},
year = {2017}
}
@book{Minsky1969,
author = {Minsky and Papert},
title = {{Perceptrons}},
year = {1969}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/neural{\_}networks/AlexNet.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/neural{\_}networks/ZFNet.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2013}
}
@misc{DigitalGlobe2017,
author = {DigitalGlobe},
title = {{Our Constellation - DigitalGlobe}},
url = {https://www.digitalglobe.com/about/our-constellation},
urldate = {2017-12-08},
year = {2017}
}
@article{Marmanis2016,
abstract = {{\textless}p{\textgreater}This paper describes a deep learning approach to semantic segmentation of very high resolution (aerial) images. Deep neural architectures hold the promise of end-to-end learning from raw images, making heuristic feature design obsolete. Over the last decade this idea has seen a revival, and in recent years deep convolutional neural networks (CNNs) have emerged as the method of choice for a range of image interpretation tasks like visual recognition and object detection. Still, standard CNNs do not lend themselves to per-pixel semantic segmentation, mainly because one of their fundamental principles is to gradually aggregate information over larger and larger image regions, making it hard to disentangle contributions from different pixels. Very recently two extensions of the CNN framework have made it possible to trace the semantic information back to a precise pixel position: deconvolutional network layers undo the spatial downsampling, and Fully Convolution Networks (FCNs) modify the fully connected classification layers of the network in such a way that the location of individual activations remains explicit. We design a FCN which takes as input intensity and range data and, with the help of aggressive deconvolution and recycling of early network layers, converts them into a pixelwise classification at full resolution. We discuss design choices and intricacies of such a network, and demonstrate that an ensemble of several networks achieves excellent results on challenging data such as the {\textless}i{\textgreater}ISPRS semantic labeling benchmark{\textless}/i{\textgreater}, using only the raw data as input.{\textless}/p{\textgreater}},
author = {Marmanis, D. and Wegner, J. D. and Galliani, S. and Schindler, K. and Datcu, M. and Stilla, U.},
doi = {10.5194/isprsannals-III-3-473-2016},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/semantic{\_}segmentation/semantic{\_}segmentation{\_}satellite.pdf:pdf},
issn = {2194-9050},
journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
pages = {473--480},
title = {{Semantic Segmentation of Aerial Images With an Ensemble of Cnns}},
url = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/III-3/473/2016/isprs-annals-III-3-473-2016.pdf},
volume = {III-3},
year = {2016}
}
@misc{Esa2009a,
author = {Esa},
title = {{50 years of Earth Observation}},
url = {http://www.esa.int/About{\_}Us/Welcome{\_}to{\_}ESA/ESA{\_}history/50{\_}years{\_}of{\_}Earth{\_}Observation},
urldate = {2017-10-24},
year = {2009}
}
@book{Patterson2017,
author = {Patterson, Josh and Gibson, Adam},
title = {{Deep Learning - A Practitioner's Approach}},
year = {2017}
}
@article{Wegner2010,
abstract = {State-of-the-art satellite SAR sensors acquire data of one meter geometric ground resolution, airborne sensors achieve even higher resolutions. Nonetheless, layover and occlusion hamper interpretability of such data particularly in urban scenes. In order to overcome this drawback, we use additional information from aerial photos to detect buildings. Features are extracted from both data sets and introduced to a common feature vector followed by a classification into building sites and non-building sites with Conditional Random Fields (CRF). Furthermore, we show that the different sensor geometries of the SAR and the optical sensor may be used to estimate building heights.},
author = {Wegner, Jan Dirk and Ziehn, Jens R. and Soergel, Uwe},
doi = {10.1109/IGARSS.2010.5653386},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/height{\_}calculation/height{\_}calculation{\_}using{\_}insar.pdf:pdf},
isbn = {9781424495658},
issn = {2153-6996},
journal = {International Geoscience and Remote Sensing Symposium (IGARSS)},
keywords = {Building detection,Classification,Fusion,Height estimation,SAR,Urban},
pages = {1928--1931},
title = {{Building detection and height estimation from high-resolution InSAR and optical data}},
year = {2010}
}
@article{Kemker2017,
abstract = {Deep convolutional neural networks (DCNNs) have been used to achieve state-of-the-art performance on many computer vision tasks (e.g., object recognition, object detection, semantic segmentation) thanks to a large repository of annotated image data. Large labeled datasets for other sensor modalities, e.g., multispectral imagery (MSI), are not available due to the large cost and manpower required. In this paper, we adapt state-of-the-art DCNN frameworks in computer vision for semantic segmentation for MSI imagery. To overcome label scarcity for MSI data, we substitute real MSI for generated synthetic MSI in order to initialize a DCNN framework. We evaluate our network initialization scheme on the new RIT-18 dataset that we present in this paper. This dataset contains very-high resolution MSI collected by an unmanned aircraft system. The models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work.},
archivePrefix = {arXiv},
arxivId = {1703.06452},
author = {Kemker, Ronald and Salvaggio, Carl and Kanan, Christopher},
eprint = {1703.06452},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/semantic{\_}segmentation/algorithms{\_}for{\_}segmantic{\_}segmentation{\_}in{\_}remote{\_}sensing{\_}using{\_}DCNNs.pdf:pdf},
keywords = {convolutional neural network,deep learning,semantic},
pages = {1--45},
title = {{Algorithms for Semantic Segmentation of Multispectral Remote Sensing Imagery using Deep Learning}},
url = {http://arxiv.org/abs/1703.06452},
year = {2017}
}
@article{Wu2017,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Wu, Songtao and Zhong, Shenghua and Liu, Yan},
doi = {10.1007/s11042-017-4440-4},
eprint = {1512.03385},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/neural{\_}networks/ResNet.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Convolutional neural networks,Image steganalysis,Residual learning},
pages = {1--17},
pmid = {23554596},
title = {{Deep residual learning for image steganalysis}},
year = {2017}
}
@article{Noh2015,
abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5{\%}) among the methods trained with no external data through ensemble with the fully convolutional network.},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
doi = {10.1109/ICCV.2015.178},
eprint = {1505.04366},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/neural{\_}networks/deconv{\_}feature{\_}extraction.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1520--1528},
pmid = {16190471},
title = {{Learning deconvolution network for semantic segmentation}},
volume = {2015 Inter},
year = {2015}
}
@article{Achanta2012,
abstract = {Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.},
author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"{u}}sstrunk, Sabine},
doi = {10.1109/TPAMI.2012.120},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/shape{\_}detection/SLIC{\_}superpixels.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Superpixels,clustering,k-means,segmentation},
number = {11},
pages = {2274--2281},
pmid = {22641706},
title = {{SLIC superpixels compared to state-of-the-art superpixel methods}},
volume = {34},
year = {2012}
}
@article{Liu2015,
abstract = {Owing to the remarkable improvements that have been made in radar sensors, it becomes possible to obtain the information for a single structure from high-resolution SAR images. In our previous research, a method for detecting the heights of low-rise buildings automatically was proposed using 2D GIS data and a single high-resolution TerraSAR-X intensity image. However, this method is difficult to apply for high-rise buildings due to surface/material conditions of their exterior walls. In this study, a new method was developed for estimating the heights of high-rise buildings based on the results from an Interferometric SAR (InSAR) analysis. The characteristics of phases in the InSAR result were investigated and used for extracting the potential layover areas of target buildings. Then, the heights were estimated according to their layover lengths. The developed method was tested on two TerraSAR-X images for San Francisco, USA, in the HighSpot mode. Comparing the result with Lidar data, the detected heights were found to be reasonable.},
author = {Liu, Wen and Suzuki, Kentaro and Yamazaki, Fumio},
doi = {10.1109/JURSE.2015.7120530},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/height{\_}calculation/sanfran{\_}height{\_}calculation{\_}using{\_}insar.pdf:pdf},
isbn = {9781479966523},
journal = {2015 Joint Urban Remote Sensing Event, JURSE 2015},
number = {June 2010},
pages = {1--4},
title = {{Height estimation for high-rise buildings based on InSAR analysis}},
year = {2015}
}
@misc{Opencv2017,
author = {Opencv},
title = {{OpenCV 2.4.13.4 documentation}},
url = {https://docs.opencv.org/2.4/index.html},
urldate = {2017-11-12},
year = {2017}
}
@article{Moon2002,
abstract = {We propose an approach to accurately detecting two-dimensional (2-D) shapes. The cross section of the shape boundary is modeled as a step function. We first derive a one-dimensional (1-D) optimal step edge operator, which minimizes both the noise power and the mean squared error between the input and the filter output. This operator is found to be the derivative of the double exponential (DODE) function, originally derived by Ben-Arie and Rao. We define an operator for shape detection by extending the DODE filter along the shape's boundary contour. The responses are accumulated at the centroid of the operator to estimate the likelihood of the presence of the given shape. This method of detecting a shape is in fact a natural extension of the task of edge detection at the pixel level to the problem of global contour detection. This simple filtering scheme also provides a tool for a systematic analysis of edge-based shape detection. We investigate how the error is propagated by the shape geometry. We have found that, under general assumptions, the operator is locally linear at the peak of the response. We compute the expected shape of the response and derive some of its statistical properties. This enables us to predict both its localization and detection performance and adjust its parameters according to imaging conditions and given performance specifications. Applications to the problem of vehicle detection in aerial images, human facial feature detection, and contour tracking in video are presented.},
author = {Moon, Hankyu and Chellappa, Rama and Rosenfeld, Azriel},
doi = {10.1109/TIP.2002.800896},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/shape{\_}detection/shape{\_}detection.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Boundary detection,Contour tracking,Edge detection,Facial feature detection,Localization,Shape detection,Step edge,Vehicle detection},
number = {11},
pages = {1209--1227},
pmid = {18249692},
title = {{Optimal edge-based shape detection}},
volume = {11},
year = {2002}
}
@misc{Karpathy2017,
author = {Karpathy},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/convolutional-networks/},
urldate = {2017-11-14},
year = {2017}
}
@article{Kaiser2017,
author = {Kaiser, Pascal and Wegner, Jan Dirk and Lucchi, Aur{\'{e}}lien and Jaggi, Martin and Hofmann, Thomas and Schindler, Konrad and Member, Senior},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/pattern{\_}recognition/learning{\_}aerial{\_}image{\_}segmentation.pdf:pdf},
pages = {1--15},
title = {{Learning Arial Image Segmentation from Online Maps}},
year = {2017}
}
@article{Pinheiro2016,
abstract = {Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse `mask encoding' in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The approach is simple, fast, and effective. Building on the recent DeepMask network for generating object proposals, we show accuracy improvements of 10-20{\%} in average recall for various setups. Additionally, by optimizing the overall network architecture, our approach, which we call SharpMask, is 50{\%} faster than the original DeepMask network (under .8s per image).},
archivePrefix = {arXiv},
arxivId = {1603.08695},
author = {Pinheiro, Pedro O. and Lin, Tsung Yi and Collobert, Ronan and Doll{\'{a}}r, Piotr},
doi = {10.1007/978-3-319-46448-0_5},
eprint = {1603.08695},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/semantic{\_}segmentation/sharpMask.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {75--91},
pmid = {4520227},
title = {{Learning to refine object segments}},
volume = {9905 LNCS},
year = {2016}
}
@article{Mcculloch1990,
abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Mcculloch, Warren S and Pitts, Walter},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/neural{\_}networks/A{\_}LOGICAL{\_}CALCULUS{\_}OF{\_}THE{\_}IDEAS{\_}IMMANENT{\_}IN{\_}NERVOUS{\_}ACTIVITY.pdf:pdf},
isbn = {0007-4985},
issn = {00074985},
journal = {Bulletin of Mathematical Biology},
pmid = {2185863},
title = {{A logical calculus of the ideas immanent in nervous activity}},
year = {1943}
}
@article{Comber2012,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Comber, Alexis and Umezaki, Masahiro and Zhou, Rena and Ding, Yongming and Li, Yang and Fu, Hua and Jiang, Hongwei and Tewkesbury, Andrew},
doi = {10.1080/01431161.2011.635161},
eprint = {arXiv:1011.1669v3},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/height{\_}calculation/using{\_}shadows{\_}in{\_}high{\_}resolution{\_}imagery{\_}to{\_}determine{\_}building{\_}height.pdf:pdf},
isbn = {9788578110796},
issn = {2150704X},
journal = {Remote Sensing Letters},
number = {7},
pages = {551--556},
pmid = {25246403},
title = {{Using shadows in high-resolution imagery to determine building height}},
volume = {3},
year = {2012}
}
@article{Zeiler2011,
abstract = {We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Zeiler, Matthew D. and Taylor, Graham W. and Fergus, Rob},
doi = {10.1109/ICCV.2011.6126474},
eprint = {1505.04366},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/neural{\_}networks/deconvolutional.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2018--2025},
title = {{Adaptive deconvolutional networks for mid and high level feature learning}},
year = {2011}
}
@article{Yu2017,
abstract = {Boundary and edge cues are highly beneficial in improving a wide variety of vision tasks such as semantic segmentation, object recognition, stereo, and object proposal generation. Recently, the problem of edge detection has been revisited and significant progress has been made with deep learning. While classical edge detection is a challenging binary problem in itself, the category-aware semantic edge detection by nature is an even more challenging multi-label problem. We model the problem such that each edge pixel can be associated with more than one class as they appear in contours or junctions belonging to two or more semantic classes. To this end, we propose a novel end-to-end deep semantic edge learning architecture based on ResNet and a new skip-layer architecture where category-wise edge activations at the top convolution layer share and are fused with the same set of bottom layer features. We then propose a multi-label loss function to supervise the fused activations. We show that our proposed architecture benefits this problem with better performance, and we outperform the current state-of-the-art semantic edge detection methods by a large margin on standard data sets such as SBD and Cityscapes.},
archivePrefix = {arXiv},
arxivId = {1705.09759},
author = {Yu, Zhiding and Feng, Chen and Liu, Ming-Yu and Ramalingam, Srikumar},
doi = {10.1109/CVPR.2017.191},
eprint = {1705.09759},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/shape{\_}detection/shape{\_}detection{\_}using{\_}casenets.pdf:pdf},
title = {{CASENet: Deep Category-Aware Semantic Edge Detection}},
url = {http://arxiv.org/abs/1705.09759},
year = {2017}
}
@misc{Airbus2017,
author = {Airbus},
title = {{Technical Information : TerraSAR-X}},
url = {http://www.intelligence-airbusds.com/en/903-technical-information},
urldate = {2017-11-19},
year = {2017}
}
@article{Lary2010,
archivePrefix = {arXiv},
arxivId = {0803973233},
author = {Lary, David J.},
doi = {10.5772/711},
eprint = {0803973233},
file = {:Users/valdemarrolfsen/Downloads/atrificial-intelligence-in-geoscience-remote-sensing.pdf:pdf},
isbn = {9789537619343},
issn = {9789533070865},
journal = {RFID Technology, Security Vulnerabilities, and Countermeasures},
keywords = {Ehi-Eromosele C.O. Nwinyi O.C. {\&} Ajani O.O.},
pages = {75--100},
pmid = {100220790},
title = {{Artificial Intelligence in Geosience and Remote Sensing}},
year = {2010}
}
@book{Worboys2003,
abstract = {GIS: A Computing Perspective, Second Edition, provides a full, up-to-date overview of the state-of-the-art in GIS, both Geographic Information Systems and the study of these systems-Geographic Information Science. Analyzing the subject from a computing perspective, the second edition explores conceptual and formal models needed to understand spatial information, and examines the representations and data structures needed to support adequate system performance. This volume also covers the special-purpose interfaces and architectures required to interact with and share spatial information, and explains the importance of uncertainty and time. The material on GIS architectures and interfaces as well as spatiotemporal information systems is almost entirely new. The second edition contains substantial new information, and has been completely reformatted to improve accessibility. Changes include: There is also a new chapter on spatial uncertaintyComplete revisions of the bibliography, index, and supporting diagramsSupplemental material is offset at the top of the page, as are references and links for further studyDefinitions of new terms are in the margins of pages where they appear, with corresponding entries in the index},
author = {Worboys, Michael F.},
booktitle = {CRC press},
isbn = {0-7484-0065-6},
issn = {0028-8144},
pages = {376},
pmid = {41376455},
title = {{GIS: A computing perspective}},
year = {2003}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/neural{\_}networks/VGGNet.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Ballard1981,
abstract = {--The Hough transform is a method for "{\~{}}.{\~{}}tecting curves by exploiting the duality between points on a curve and parameters of that curve. The initial work showed how to detect both analytic curves (1'2) and non-analytic curves, (3) but these methods were restricted to binary edge images. This work was generalized to the detection of some analytic curves in grey level images, specifically lines, 14) circles 15) and parabolas/6) The line detection case is the best known of these and has been ingeniously exploited in several applications/7'89) We show how the boundaries of an arbitrary non-analytic shape can be used to construct a mapping between image space and Hough transform space. Such a mapping can be exploited to detect instances of that particular shape in an image. Furthermore, variations in the shape such as rotations, scale changes or figure-ground reversals correspond to straightforward transformations of this mapping. However, the most remarkable property is that such mappings can be composed to build mappings for complex shapes from the mappings of simpler component shapes. This makes the generalized Hough transform a kind of universal transform which can be used to find arbitrarily complex shapes.},
author = {Ballard, D H},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/shape{\_}detection/GENERALIZING{\_}THE{\_}HOUGH{\_}TRANSFORM.pdf:pdf},
journal = {Pattern Recoqnition},
keywords = {Hough transform,Image processing,Parallel algorithms,Pattern recognition,Shape recognition},
number = {2},
pages = {111--122},
title = {{Generalizing the Hough Transform To Detect Arbitrary Shapes*}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.447.7083{\&}rep=rep1{\&}type=pdf},
volume = {13},
year = {1981}
}
@article{Lin2016,
abstract = {Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.},
archivePrefix = {arXiv},
arxivId = {1611.06612},
author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
doi = {10.1109/CVPR.2017.549},
eprint = {1611.06612},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/semantic{\_}segmentation/refineNet.pdf:pdf},
isbn = {978-1-5386-0457-1},
title = {{RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation}},
url = {http://arxiv.org/abs/1611.06612},
year = {2016}
}
@article{Szegedy2014,
abstract = {We propose a deep convolutional neural network ar- chitecture codenamed Inception that achieves the new state of the art for classification and detection in the Im- ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the compu- tational budget constant. To optimize quality, the architec- tural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in- carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and Hill, Chapel and Arbor, Ann},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/neural{\_}networks/GoogLeNet.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {1--9},
pmid = {24920543},
title = {{Going Deeper with Convolutions}},
year = {2014}
}
@misc{Planet2017,
author = {Planet},
title = {{SkySat - Constellations}},
url = {https://www.planet.com/products/hi-res-monitoring/},
urldate = {2017-12-08},
year = {2017}
}
@article{Jo2012,
abstract = {State-of-the-art smart sensor technology enables deployment of dense arrays of sensors, which is critical for structural health monitoring (SHM) of complicated and large-scale civil structures. Despite recent successful implementation of various wireless smart sensor networks (WSSNs) for full-scale SHM, the low-cost micro-electro-mechanical systems (MEMS) sensors commonly used in smart sensors cannot readily measure low-level ambient vibrations because of their relatively low resolution. Combined use of conventional wired high-sensitivity sensors with low-cost wireless smart sensors has been shown to provide improved spectral estimates of response that can lead to improved experimental modal analysis. However, such a heterogeneous network of wired and wireless sensors requires central collection of an enormous amount of raw data and off-network processing to achieve global time synchronization; consequently, many of the advantages of WSSNs for SHM are lost. In this paper, the development of a new high-sensitivity accelerometer board (SHM-H) for the Imote2 wireless smart sensor (WSS) platform is presented. The use of a small number of these high-sensitivity WSSs, composed of the SHM-H and Imote2, as reference sensors in the Natural Excitation Technique—based decentralized WSSN strategy is explored and is shown to provide a cost-effective means of improving modal feature extraction in the decentralized WSSN for SHM. Read More: http://ascelibrary.org.proxy.lib.umich.edu/doi/abs/10.1061/{\%}28ASCE{\%}29EM.1943-7889.0000352},
author = {Yu, Tzu-Yang},
doi = {10.1061/(ASCE)EM.1943-7889},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/height{\_}calculation/sar.pdf:pdf},
issn = {0733-9399},
journal = {Journal of Engineering Mechanics},
number = {6},
pages = {683--694},
title = {{Development and Application of High-Sensitivity Wireless Smart Sensors for Decentralized Stochastic Modal Identification}},
url = {http://ascelibrary.org/doi/abs/10.1061/(ASCE)EM.1943-7889.0000352},
volume = {138},
year = {2012}
}
@article{LeC,
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {{Le Cun}, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/neural{\_}networks/LeNet.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
pmid = {15823584},
title = {{Gradient-Based Learning Applied to Document Recognition}},
year = {1998}
}
@article{Brunner2008,
author = {Brunner, Dominik and Lemoine, Guido and Bruzzone, Lorenzo},
doi = {10.1117/12.800736},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/height{\_}calculation/height{\_}retrival{\_}using{\_}sar.pdf:pdf},
keywords = {3d reconstruction,building detection,height extraction,vhr sar},
number = {3},
pages = {71100F--71100F--12},
title = {{Building Height Retrieval From VHR SAR Imagery Based on an Iterative Simulation and Matching Technique}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=796367},
volume = {7110},
year = {2008}
}
@misc{Fisher2003,
author = {Fisher, R and Perkins, S and Walker, A and Wolfard, E},
title = {{Image Transforms - Hough Transform}},
url = {https://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm},
urldate = {2017-12-06},
year = {2003}
}
@misc{Cambridge2017,
author = {Cambridge},
title = {{Convolutional Neural Networks with Keras}},
url = {https://cambridgespark.com/content/tutorials/convolutional-neural-networks-with-keras/index.html},
urldate = {2017-11-14},
year = {2017}
}
@article{Pinheiro2016,
abstract = {Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse `mask encoding' in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The approach is simple, fast, and effective. Building on the recent DeepMask network for generating object proposals, we show accuracy improvements of 10-20{\%} in average recall for various setups. Additionally, by optimizing the overall network architecture, our approach, which we call SharpMask, is 50{\%} faster than the original DeepMask network (under .8s per image).},
archivePrefix = {arXiv},
arxivId = {1603.08695},
author = {Pinheiro, Pedro O. and Lin, Tsung Yi and Collobert, Ronan and Doll{\'{a}}r, Piotr},
doi = {10.1007/978-3-319-46448-0_5},
eprint = {1603.08695},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/semantic{\_}segmentation/sharpMask.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {75--91},
pmid = {4520227},
title = {{Learning to refine object segments}},
volume = {9905 LNCS},
year = {2016}
}
@article{Lary2016,
abstract = {Learning incorporates a broad range of complex procedures. Machine learning (ML) is a subdivision of artificial intelligence based on the biological learning process. The ML approach deals with the design of algorithms to learn from machine readable data. ML covers main domains such as data mining, difficult-to-program applications, and software applications. It is a collection of a variety of algorithms (e.g. neural networks, support vector machines, self-organizing map, decision trees, random forests, case-based reasoning, genetic programming, etc.) that can provide multivariate, nonlinear, nonparametric regression or classification. The modeling capabilities of the ML-based methods have resulted in their extensive applications in science and engineering. Herein, the role of ML as an effective approach for solving problems in geosciences and remote sensing will be highlighted. The unique features of some of the ML techniques will be outlined with a specific attention to genetic programming paradigm. Furthermore, nonparametric regression and classification illustrative examples are presented to demonstrate the efficiency of ML for tackling the geosciences and remote sensing problems.},
author = {Lary, David J. and Alavi, Amir H. and Gandomi, Amir H. and Walker, Annette L.},
doi = {10.1016/j.gsf.2015.07.003},
file = {:Users/valdemarrolfsen/Downloads/machine-learning-in-remote-sensing.pdf:pdf},
isbn = {16749871},
issn = {16749871},
journal = {Geoscience Frontiers},
keywords = {Classification,Geosciences,Machine learning,Regression,Remote sensing},
number = {1},
pages = {3--10},
publisher = {Elsevier Ltd},
title = {{Machine learning in geosciences and remote sensing}},
url = {http://dx.doi.org/10.1016/j.gsf.2015.07.003},
volume = {7},
year = {2016}
}
@article{Shao2011,
abstract = {The spectral confusion between shadow and water (or other dark surfaces) often results in suboptimal urban classification performances, especially from high-resolution satellite imagery (e.g. IKONOS). A classification method was developed to incorporate spatial indices of image objects to improve the shadow/water detection. A number of spatial indices, such as size, shape and spatial neighbour of image objects, were characterized to differentiate water and shadow objects. This generated superior shadow/water detection performance compared to a traditional per-field Extraction and Classification of Homogeneous Objects (ECHO) classification method. The user's accuracies for shadow and water classes were increased to 88{\%} and 92{\%}, compared to 80{\%} and 76{\%} obtained from the traditional ECHO classification approach. Furthermore, an automated approach was developed for shadow-length and corresponding building-height estimation. The accuracy assessment suggested good results for very high buildings, especially for...},
author = {Shao, Yang and Taff, Gregoryn N. and Walsh, Stephen J.},
doi = {10.1080/01431161.2010.517226},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/height{\_}calculation/shadow{\_}detection{\_}and{\_}building{\_}height{\_}estimation{\_}using{\_}IKONOS{\_}data.pdf:pdf},
issn = {01431161},
journal = {International Journal of Remote Sensing},
number = {22},
pages = {6929--6944},
title = {{Shadow detection and building-height estimation using IKONOS data}},
volume = {32},
year = {2011}
}
@article{Lin2016,
abstract = {Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.},
archivePrefix = {arXiv},
arxivId = {1611.06612},
author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
doi = {10.1109/CVPR.2017.549},
eprint = {1611.06612},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/semantic{\_}segmentation/refineNet.pdf:pdf},
isbn = {978-1-5386-0457-1},
title = {{RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation}},
url = {http://arxiv.org/abs/1611.06612},
year = {2016}
}
@misc{AirBus2017,
author = {AirBus},
title = {{Technical Information : Airbus Defence and Space}},
url = {http://www.intelligence-airbusds.com/en/903-technical-information},
urldate = {2017-12-11},
year = {2017}
}
@misc{Frossard2016,
author = {Frossard, Davi},
title = {{VGG in TensorFlow}},
url = {https://www.cs.toronto.edu/{~}frossard/post/vgg16/},
urldate = {2017-11-16},
year = {2016}
}
@misc{GISLounge2014,
author = {GISLounge},
title = {{Measuring Object Heights from Satellite Imagery}},
url = {https://www.gislounge.com/shadows-angles-measuring-object-heights-satellite-imagery/},
urldate = {2017-11-18},
year = {2014}
}
@article{Ballard1981,
abstract = {--The Hough transform is a method for "{\~{}}.{\~{}}tecting curves by exploiting the duality between points on a curve and parameters of that curve. The initial work showed how to detect both analytic curves (1'2) and non-analytic curves, (3) but these methods were restricted to binary edge images. This work was generalized to the detection of some analytic curves in grey level images, specifically lines, 14) circles 15) and parabolas/6) The line detection case is the best known of these and has been ingeniously exploited in several applications/7'89) We show how the boundaries of an arbitrary non-analytic shape can be used to construct a mapping between image space and Hough transform space. Such a mapping can be exploited to detect instances of that particular shape in an image. Furthermore, variations in the shape such as rotations, scale changes or figure-ground reversals correspond to straightforward transformations of this mapping. However, the most remarkable property is that such mappings can be composed to build mappings for complex shapes from the mappings of simpler component shapes. This makes the generalized Hough transform a kind of universal transform which can be used to find arbitrarily complex shapes.},
author = {Ballard, D H},
file = {:Users/valdemarrolfsen/Documents/prosjektoppgave/sources/shape{\_}detection/GENERALIZING{\_}THE{\_}HOUGH{\_}TRANSFORM.pdf:pdf},
journal = {Pattern Recoqnition},
keywords = {Hough transform,Image processing,Parallel algorithms,Pattern recognition,Shape recognition},
number = {2},
pages = {111--122},
title = {{Generalizing the Hough Transform To Detect Arbitrary Shapes*}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.447.7083{\&}rep=rep1{\&}type=pdf},
volume = {13},
year = {1981}
}
